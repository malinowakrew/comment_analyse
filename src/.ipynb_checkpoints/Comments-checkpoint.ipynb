{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking comments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to upload date form the page. We use follow libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 'https://www.horkruks.com/'\n",
    "url = requests.get(page)\n",
    "code = BeautifulSoup(url.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the bottom there is a list of links to another pages (archivest). We are going to place them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = code.findAll(\"div\", {\"id\": \"BlogArchive1_ArchiveList\"})\n",
    "lista = lista[0].findAll(\"option\")\n",
    "\n",
    "pages = pd.Series()\n",
    "for http in lista:\n",
    "    pages.at[len(pages)] = (re.sub(\"'\", \"\", str(http['value'])))\n",
    "    \n",
    "#write pages\n",
    "#pages.to_csv('pages_horkruks_links')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fisrt link is empty so we delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages = pages.drop([0]) \n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in every link we want to collect all links to particular posts. Well, we start reading links like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_page = pd.Series()\n",
    "for page in pages:\n",
    "    url = requests.get(page)\n",
    "    page_code = BeautifulSoup(url.text, \"html.parser\")\n",
    "    single_page.at[len(single_page)] = page_code.findAll(\"h3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parse every link and then take more links from it. We take only a random sample of list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_page_list = list(single_page)\n",
    "from random import sample\n",
    "random_sample = sample(single_page_list, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taka wstawka zupełnie niepotrzebna\n",
    "url = requests.get(pages[9])\n",
    "code = BeautifulSoup(url.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_single_link = pd.Series()\n",
    "\n",
    "for record in random_sample:\n",
    "    for item in record:\n",
    "        list_of_a = item.findAll(\"a\", href = True)\n",
    "        for a in list_of_a:\n",
    "            list_of_single_link.at[len(list_of_single_link)] = a['href']\n",
    "            \n",
    "list_of_single_link = list_of_single_link.sample(frac=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a list to partucular posts as we want. We can make a frame of comments and its authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_text = []\n",
    "comment_author = []\n",
    "comment_tittle = []\n",
    "\n",
    "for item in list_of_single_link:\n",
    "    url = requests.get(item)\n",
    "    page_code = BeautifulSoup(url.text, \"html.parser\")\n",
    "    \n",
    "    page_name = re.sub(\"https://www.horkruks.com/\", \"\", item)\n",
    "    page_name = re.sub(\".html\", \"\", page_name)\n",
    "\n",
    "    for i in (page_code.findAll(\"cite\", {\"class\":\"user\"})):\n",
    "        comment_author.append(i.text)\n",
    "        comment_tittle.append(page_name)\n",
    "        \n",
    "    for i in (page_code.findAll(\"p\", {\"class\": \"comment-content\"})):\n",
    "        comment_text.append(i.text)\n",
    "\n",
    "comment_data = pd.DataFrame()\n",
    "comment_data['text'] = pd.Series(comment_text)\n",
    "comment_data['user'] = pd.Series(comment_author)\n",
    "comment_data['tittle'] = pd.Series(comment_tittle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check most cammon authors of comments. Usually, it's only Anonymous person or a owner of the page Laura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anonymous        184\n",
      "Laura            100\n",
      "Unknown            7\n",
      "Ev DAILY           3\n",
      "Jeans Please!      3\n",
      "Name: user, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "the_most_common_authors = comment_data['user'].value_counts()\n",
    "print(the_most_common_authors.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write frame to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data.to_csv(r'C:\\Users\\edzia\\OneDrive\\Desktop\\Horkruks\\dane_with_tittle.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 - sentiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use \"tittle\" column. We need to know on where the comment belongs because we want to measure how many of posts are good. We don't want to measure the sentiment of author of blog so we throw Laura's comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data = pd.read_csv(r'C:\\Users\\edzia\\OneDrive\\Desktop\\Horkruks\\dane_with_tittle.csv')\n",
    "fans_comment_data = comment_data.where(comment_data['user'] != \"Laura\")\n",
    "fans_comment_data = fans_comment_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can clean every comment from interpunction. We are not going to use it in out test.\n",
    "First we use string dictionary to get a list of punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = []\n",
    "for comment in fans_comment_data['text']:\n",
    "        clean.append(\"\".join([word for word in comment if word not in punctuation]))\n",
    "fans_comment_data['clean_text'] = pd.Series(clean)\n",
    "\n",
    "#   corpus [i] = re.sub(r'\\W',' ',corpus [i])\n",
    "#   corpus [i] = re.sub(r'\\s+',' ',corpus [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czy pojawią się jakieś propozycje studniówkowe\n",
      "Laura jaki masz rozmiar tych spodni dresowych z branstore.com? \n",
      "     Unnamed: 0                                               text  \\\n",
      "0           0.0     odpowiesz dzisiaj na maile? :)pytałam o kurtkę   \n",
      "1           1.0                                           mistrzu!   \n",
      "2           2.0  Laura pokaż jakie podobaja Ci sie kreski eyeli...   \n",
      "3           3.0  Laurka, co myślisz? http://s33.dawandastatic.c...   \n",
      "4           4.0  Hej Laura, bede na dniach zamawiac clubmastery...   \n",
      "..          ...                                                ...   \n",
      "214       214.0                  Hej Lori wracaj tu kochana :*****   \n",
      "215       215.0  Dzieki Ci za ten post. Jestes chyba jedyna blo...   \n",
      "216       216.0               L, co sądzisz o weganizmie/weganach?   \n",
      "217       217.0                        Podpinam się pod pytanko...   \n",
      "219       219.0                          To gumka do włosów moye?    \n",
      "\n",
      "              user                              tittle  \\\n",
      "0        Anonymous           2015/03/beige-light-denim   \n",
      "1    Jeans Please!           2015/03/beige-light-denim   \n",
      "2        Anonymous           2015/03/beige-light-denim   \n",
      "3        Anonymous           2015/03/beige-light-denim   \n",
      "4        Anonymous           2015/03/beige-light-denim   \n",
      "..             ...                                 ...   \n",
      "214      Anonymous       2013/02/jw-anderson-fall-2013   \n",
      "215      Anonymous  2018/11/moschino-x-h-leather-pants   \n",
      "216      Anonymous  2018/11/moschino-x-h-leather-pants   \n",
      "217         fluorr  2018/11/moschino-x-h-leather-pants   \n",
      "219      Anonymous  2018/11/moschino-x-h-leather-pants   \n",
      "\n",
      "                                            clean_text  \n",
      "0          odpowiesz dzisiaj na maile pytałam o kurtkę  \n",
      "1                                              mistrzu  \n",
      "2    Laura pokaż jakie podobaja Ci sie kreski eyeli...  \n",
      "3    Laurka co myślisz https33dawandastaticcomProdu...  \n",
      "4    Hej Laura bede na dniach zamawiac clubmastery ...  \n",
      "..                                                 ...  \n",
      "214  Czy masz gdzieś rozstępy na ciele Nie sugeruję...  \n",
      "215  httpwwwhorkrukscom201704yvessaintlaurentmonpar...  \n",
      "216  W których dokładnie miejscach jesteś ostrzykni...  \n",
      "217  Można wnioskować  usta łuki brwiowe żuchwa ja ...  \n",
      "219  Stunning colourYour nails are gorgeousKiriSet ...  \n",
      "\n",
      "[151 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(fans_comment_data['clean_text'].iloc[29])\n",
    "print(fans_comment_data['text'].iloc[30])\n",
    "fans_comment_data = fans_comment_data.dropna()\n",
    "print(fans_comment_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To jest i 0, to jest j 1, to jest k ('piękny', 'piękny', 'adj:sg:acc:m3:pos', [], [])\n",
      "piękny\n",
      "piękny\n",
      "adj:sg:acc:m3:pos\n",
      "[]\n",
      "[]\n",
      "To jest i 0, to jest j 1, to jest k ('piękny', 'piękny', 'adj:sg:nom.voc:m1.m2.m3:pos', [], [])\n",
      "piękny\n",
      "piękny\n",
      "adj:sg:nom.voc:m1.m2.m3:pos\n",
      "[]\n",
      "[]\n",
      "To jest i 1, to jest j 2, to jest k ('dzień', 'dzień:s1', 'subst:sg:nom.acc:m3', ['nazwa_pospolita'], [])\n",
      "dzień\n",
      "dzień:s1\n",
      "subst:sg:nom.acc:m3\n",
      "['nazwa_pospolita']\n",
      "[]\n",
      "To jest i 1, to jest j 2, to jest k ('dzień', 'dzień:s2', 'subst:sg:nom:f', ['nazwa_pospolita'], ['daw.', 'pszcz.'])\n",
      "dzień\n",
      "dzień:s2\n",
      "subst:sg:nom:f\n",
      "['nazwa_pospolita']\n",
      "['daw.', 'pszcz.']\n",
      "To jest i 1, to jest j 2, to jest k ('dzień', 'dzień:s2', 'subst:sg:acc:f', ['nazwa_pospolita'], ['daw.', 'pszcz.'])\n",
      "dzień\n",
      "dzień:s2\n",
      "subst:sg:acc:f\n",
      "['nazwa_pospolita']\n",
      "['daw.', 'pszcz.']\n",
      "To jest i 1, to jest j 2, to jest k ('dzień', 'dzienia', 'subst:pl:gen:f', ['nazwa_pospolita'], ['char.', 'daw.'])\n",
      "dzień\n",
      "dzienia\n",
      "subst:pl:gen:f\n",
      "['nazwa_pospolita']\n",
      "['char.', 'daw.']\n",
      "To jest i 1, to jest j 2, to jest k ('dzień', 'dzienie', 'subst:pl:gen:n:ncol', ['nazwa_pospolita'], ['daw.'])\n",
      "dzień\n",
      "dzienie\n",
      "subst:pl:gen:n:ncol\n",
      "['nazwa_pospolita']\n",
      "['daw.']\n",
      "To jest i 1, to jest j 2, to jest k ('dzień', 'dzienić', 'impt:sg:sec:imperf', [], ['daw.'])\n",
      "dzień\n",
      "dzienić\n",
      "impt:sg:sec:imperf\n",
      "[]\n",
      "['daw.']\n",
      "To jest i 2, to jest j 3, to jest k ('dziś', 'dziś:d', 'adv', [], [])\n",
      "dziś\n",
      "dziś:d\n",
      "adv\n",
      "[]\n",
      "[]\n",
      "To jest i 2, to jest j 3, to jest k ('dziś', 'dziś:s', 'subst:sg.pl:nom.gen.dat.acc.inst.loc.voc:n:ncol', ['nazwa_pospolita'], [])\n",
      "dziś\n",
      "dziś:s\n",
      "subst:sg.pl:nom.gen.dat.acc.inst.loc.voc:n:ncol\n",
      "['nazwa_pospolita']\n",
      "[]\n",
      "To jest i 3, to jest j 4, to jest k ('nam', 'my', 'ppron12:pl:dat:m1.m2.m3.f.n:pri', [], [])\n",
      "nam\n",
      "my\n",
      "ppron12:pl:dat:m1.m2.m3.f.n:pri\n",
      "[]\n",
      "[]\n",
      "To jest i 4, to jest j 5, to jest k ('nastał', 'nastać:v1', 'praet:sg:m1.m2.m3:perf', [], [])\n",
      "nastał\n",
      "nastać:v1\n",
      "praet:sg:m1.m2.m3:perf\n",
      "[]\n",
      "[]\n",
      "To jest i 4, to jest j 5, to jest k ('nastał', 'nastać:v2', 'praet:sg:m1.m2.m3:perf', [], [])\n",
      "nastał\n",
      "nastać:v2\n",
      "praet:sg:m1.m2.m3:perf\n",
      "[]\n",
      "[]\n",
      "To jest i 5, to jest j 6, to jest k ('.', '.', 'interp', [], [])\n",
      ".\n",
      ".\n",
      "interp\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import morfeusz2\n",
    "check = morfeusz2.Morfeusz()\n",
    "analysis = check.analyse(\"piękny dzień dziś nam nastał.\")\n",
    "for i, j, k in analysis:\n",
    "    print(f\"To jest i {i}, to jest j {j}, to jest k {k}\")\n",
    "    for item in k:\n",
    "        print(item)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     https://www.horkruks.com/2019/12/\n",
      "2     https://www.horkruks.com/2019/08/\n",
      "3     https://www.horkruks.com/2019/07/\n",
      "4     https://www.horkruks.com/2019/06/\n",
      "5     https://www.horkruks.com/2019/05/\n",
      "                    ...                \n",
      "89    https://www.horkruks.com/2012/04/\n",
      "90    https://www.horkruks.com/2012/03/\n",
      "91    https://www.horkruks.com/2012/01/\n",
      "92    https://www.horkruks.com/2011/12/\n",
      "93    https://www.horkruks.com/2011/11/\n",
      "Length: 93, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_csv.reader at 0x222b84a0458>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n",
      "343\n",
      "71\n",
      "234\n",
      "22\n",
      "69\n",
      "206\n"
     ]
    }
   ],
   "source": [
    "for it in list_of_single_link.index:\n",
    "    print(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_csv.reader object at 0x00000222B84A01E8>\n"
     ]
    }
   ],
   "source": [
    "print(comment_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Anonymous                  190\n",
       "Unknown                      8\n",
       "Anna-RUCIAPRADA              2\n",
       "Kasia P                      2\n",
       "Ev DAILY                     2\n",
       "alessa                       2\n",
       "MILEX                        2\n",
       "KatLee                       2\n",
       "TynkaaBlog.                  2\n",
       "dwietwarzedwaspojrzenia      1\n",
       "moiminnie                    1\n",
       "Kuba                         1\n",
       "Tereza N.                    1\n",
       "Limuzyna-Warszawa.pl         1\n",
       "Angieness                    1\n",
       "Z Sodomy do Gomory           1\n",
       "bloo90fashion                1\n",
       "Muse x Malu                  1\n",
       "kokijaze                     1\n",
       "Aneta                        1\n",
       "Lukrecja                     1\n",
       "BUBBLEGUM VENOM              1\n",
       "Zaw.                         1\n",
       "GRAPHITREE                   1\n",
       "K.                           1\n",
       "SinsofBlack                  1\n",
       "Enigma                       1\n",
       "fuck fuckin' fuck            1\n",
       "ayayay                       1\n",
       "SIMON                        1\n",
       "OdiousStyle                  1\n",
       "Magdalena                    1\n",
       "Marta                        1\n",
       "Martyna                      1\n",
       "ROCCA COCCA                  1\n",
       "Dominika                     1\n",
       "Name: user, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fans_comment_data[\"user\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
